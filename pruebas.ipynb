{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6ff645",
   "metadata": {},
   "source": [
    "En este notebook vamos a armar y entrenar los primeros modelos en base al set creado en EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0516d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import metrics\n",
    "from src import plots\n",
    "from src import preprocessing\n",
    "from src import models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d50a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed/monaco_2025_colapinto_alllaps.csv\")\n",
    "\n",
    "# Target\n",
    "y = df[\"LapTime_s\"].to_numpy()\n",
    "\n",
    "# Features legales\n",
    "LEGAL_FEATURES_NUM = [\"LapNumber\", \"Stint\", \"TyreLife\"]\n",
    "LEGAL_FEATURES_CAT = [\"Session\", \"Compound\"]\n",
    "\n",
    "LEGAL_FEATURES_NUM = [c for c in LEGAL_FEATURES_NUM if c in df.columns]\n",
    "LEGAL_FEATURES_CAT = [c for c in LEGAL_FEATURES_CAT if c in df.columns]\n",
    "\n",
    "X = df[LEGAL_FEATURES_NUM + LEGAL_FEATURES_CAT].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d1e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), LEGAL_FEATURES_NUM),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), LEGAL_FEATURES_CAT),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353256e",
   "metadata": {},
   "source": [
    "Analisis de NaNs en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14038bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LapNumber</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stint</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TyreLife</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Session</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Compound</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           missing_count  missing_%\n",
       "LapNumber              0        0.0\n",
       "Stint                  0        0.0\n",
       "TyreLife               0        0.0\n",
       "Session                0        0.0\n",
       "Compound               0        0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES = LEGAL_FEATURES_NUM + LEGAL_FEATURES_CAT\n",
    "\n",
    "missing_counts = df[FEATURES].isna().sum()\n",
    "missing_percent = df[FEATURES].isna().mean() * 100\n",
    "\n",
    "na_summary = pd.DataFrame({\n",
    "    \"missing_count\": missing_counts,\n",
    "    \"missing_%\": missing_percent.round(2)\n",
    "}).sort_values(\"missing_%\", ascending=False)\n",
    "\n",
    "na_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9701dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in FEATURES:\n",
    "    na_mask = df[col].isna()\n",
    "    if na_mask.any():\n",
    "        print(f\"\\nColumna: {col}\")\n",
    "        print(df.loc[na_mask, \"Session\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9fe91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No hay NaN en la columna LapNumber\n",
      "\n",
      "No hay NaN en la columna Stint\n",
      "\n",
      "No hay NaN en la columna TyreLife\n",
      "\n",
      "No hay NaN en la columna Session\n",
      "\n",
      "No hay NaN en la columna Compound\n"
     ]
    }
   ],
   "source": [
    "cols_context = [\"Session\", \"LapNumber\", \"Stint\", \"Compound\", \"TyreLife\"]\n",
    "\n",
    "for col in FEATURES:\n",
    "    na_mask = df[col].isna()\n",
    "    if na_mask.any():\n",
    "        print(f\"\\n=== Ejemplos de filas con NaN en {col} ===\")\n",
    "        display(df.loc[na_mask, cols_context].head(10))\n",
    "    else:\n",
    "        print(f\"\\nNo hay NaN en la columna {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe66cda",
   "metadata": {},
   "source": [
    "Hacemos CrossValidation con K-Fold para poder tener una mejor evaluacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76dbec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(name, regressor, X, y, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo de regresión usando KFold CV.\n",
    "    Devuelve un dict con métricas promedio.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    mae_scores = []\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), start=1):\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test  = X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test  = y[test_idx]\n",
    "        \n",
    "        # Nuevo pipeline para este fold\n",
    "        reg = clone(regressor)\n",
    "        model = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"regressor\", reg),\n",
    "        ])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mae = metrics.MAE(y_test, y_pred)\n",
    "        rmse = metrics.RMSE(y_test, y_pred)\n",
    "        r2 = metrics.R2(y_test, y_pred)\n",
    "        \n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        print(f\"[{name}] Fold {fold}: MAE={mae:.3f}, RMSE={rmse:.3f}, R2={r2:.3f}\")\n",
    "    \n",
    "    mae_mean, mae_std = np.mean(mae_scores), np.std(mae_scores)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_scores), np.std(rmse_scores)\n",
    "    r2_mean, r2_std = np.mean(r2_scores), np.std(r2_scores)\n",
    "    \n",
    "    print(f\"\\n[{name}] === Promedio {n_splits} folds ===\")\n",
    "    print(f\"MAE  medio: {mae_mean:.3f} ± {mae_std:.3f}\")\n",
    "    print(f\"RMSE medio: {rmse_mean:.3f} ± {rmse_std:.3f}\")\n",
    "    print(f\"R2   medio: {r2_mean:.3f} ± {r2_std:.3f}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"MAE_mean\": mae_mean,\n",
    "        \"MAE_std\": mae_std,\n",
    "        \"RMSE_mean\": rmse_mean,\n",
    "        \"RMSE_std\": rmse_std,\n",
    "        \"R2_mean\": r2_mean,\n",
    "        \"R2_std\": r2_std,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a85dba",
   "metadata": {},
   "source": [
    "Defino 4 Primeros modelos para seleccionar uno como Baseline y poder realizar Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e238c5",
   "metadata": {},
   "source": [
    "Entreno un RandomForest, un GradientBoosting, un Riedge y un MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96496dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "\n",
    "    \"MLP\": MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e40a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] Fold 1: MAE=0.936, RMSE=1.346, R2=0.657\n",
      "[RandomForest] Fold 2: MAE=0.386, RMSE=0.466, R2=0.965\n",
      "[RandomForest] Fold 3: MAE=0.718, RMSE=1.009, R2=0.823\n",
      "[RandomForest] Fold 4: MAE=0.954, RMSE=1.353, R2=0.529\n",
      "[RandomForest] Fold 5: MAE=0.557, RMSE=0.709, R2=0.825\n",
      "\n",
      "[RandomForest] === Promedio 5 folds ===\n",
      "MAE  medio: 0.710 ± 0.219\n",
      "RMSE medio: 0.977 ± 0.350\n",
      "R2   medio: 0.760 ± 0.151\n",
      "\n",
      "[GradientBoosting] Fold 1: MAE=0.955, RMSE=1.306, R2=0.677\n",
      "[GradientBoosting] Fold 2: MAE=0.370, RMSE=0.461, R2=0.966\n",
      "[GradientBoosting] Fold 3: MAE=0.713, RMSE=0.990, R2=0.829\n",
      "[GradientBoosting] Fold 4: MAE=0.983, RMSE=1.366, R2=0.520\n",
      "[GradientBoosting] Fold 5: MAE=0.501, RMSE=0.621, R2=0.866\n",
      "\n",
      "[GradientBoosting] === Promedio 5 folds ===\n",
      "MAE  medio: 0.704 ± 0.242\n",
      "RMSE medio: 0.949 ± 0.360\n",
      "R2   medio: 0.772 ± 0.156\n",
      "\n",
      "[Ridge] Fold 1: MAE=1.074, RMSE=1.360, R2=0.650\n",
      "[Ridge] Fold 2: MAE=0.752, RMSE=0.979, R2=0.847\n",
      "[Ridge] Fold 3: MAE=1.048, RMSE=1.300, R2=0.706\n",
      "[Ridge] Fold 4: MAE=1.259, RMSE=1.622, R2=0.324\n",
      "[Ridge] Fold 5: MAE=0.742, RMSE=1.091, R2=0.586\n",
      "\n",
      "[Ridge] === Promedio 5 folds ===\n",
      "MAE  medio: 0.975 ± 0.200\n",
      "RMSE medio: 1.270 ± 0.223\n",
      "R2   medio: 0.623 ± 0.173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Archivos\\UdeSA\\IngIA\\2025\\Machine Learning\\TpFinal-F1\\F1_Pini_Carruthers\\pytorch_cpu_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Fold 1: MAE=3.561, RMSE=4.241, R2=-2.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Archivos\\UdeSA\\IngIA\\2025\\Machine Learning\\TpFinal-F1\\F1_Pini_Carruthers\\pytorch_cpu_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Fold 2: MAE=2.457, RMSE=3.047, R2=-0.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Archivos\\UdeSA\\IngIA\\2025\\Machine Learning\\TpFinal-F1\\F1_Pini_Carruthers\\pytorch_cpu_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Fold 3: MAE=2.102, RMSE=2.919, R2=-0.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Archivos\\UdeSA\\IngIA\\2025\\Machine Learning\\TpFinal-F1\\F1_Pini_Carruthers\\pytorch_cpu_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Fold 4: MAE=3.689, RMSE=4.472, R2=-4.143\n",
      "[MLP] Fold 5: MAE=2.690, RMSE=3.153, R2=-2.457\n",
      "\n",
      "[MLP] === Promedio 5 folds ===\n",
      "MAE  medio: 2.900 ± 0.622\n",
      "RMSE medio: 3.566 ± 0.653\n",
      "R2   medio: -1.993 ± 1.384\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Archivos\\UdeSA\\IngIA\\2025\\Machine Learning\\TpFinal-F1\\F1_Pini_Carruthers\\pytorch_cpu_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, reg in models.items():\n",
    "    res = evaluate_model_cv(name, reg, X, y, n_splits=5, random_state=42)\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf413",
   "metadata": {},
   "source": [
    "Resumen de las metricas de los Modelos evaluados por Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d3807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.710093</td>\n",
       "      <td>0.218863</td>\n",
       "      <td>0.976712</td>\n",
       "      <td>0.349585</td>\n",
       "      <td>0.759938</td>\n",
       "      <td>0.151176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.704468</td>\n",
       "      <td>0.242255</td>\n",
       "      <td>0.949001</td>\n",
       "      <td>0.360163</td>\n",
       "      <td>0.771717</td>\n",
       "      <td>0.156358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.974944</td>\n",
       "      <td>0.200098</td>\n",
       "      <td>1.270406</td>\n",
       "      <td>0.223421</td>\n",
       "      <td>0.622526</td>\n",
       "      <td>0.172634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>2.899803</td>\n",
       "      <td>0.622210</td>\n",
       "      <td>3.566432</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>-1.993208</td>\n",
       "      <td>1.384126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  MAE_mean   MAE_std  RMSE_mean  RMSE_std   R2_mean  \\\n",
       "0      RandomForest  0.710093  0.218863   0.976712  0.349585  0.759938   \n",
       "1  GradientBoosting  0.704468  0.242255   0.949001  0.360163  0.771717   \n",
       "2             Ridge  0.974944  0.200098   1.270406  0.223421  0.622526   \n",
       "3               MLP  2.899803  0.622210   3.566432  0.653400 -1.993208   \n",
       "\n",
       "     R2_std  \n",
       "0  0.151176  \n",
       "1  0.156358  \n",
       "2  0.172634  \n",
       "3  1.384126  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4526c9c",
   "metadata": {},
   "source": [
    "Seleccion de Modelo Baseline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4d6d8",
   "metadata": {},
   "source": [
    "GradientBoosting es el mejor basicamente en todas las metricas o muy similares a las del RF. La diferencia no es enorme, pero si tengo que elegir uno, el GB gana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c49a1",
   "metadata": {},
   "source": [
    "- Tiende a generalizar un poco mejor,\n",
    "- Suele ser más sensible a pequeños cambios de features (bueno para ver el efecto del feature engineering).\n",
    "- Es buen candidato para tunear despues ya que puedo jugar con n_estimators, learning_rate, max_depth, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259d19c",
   "metadata": {},
   "source": [
    "Con respecto al resto de modelos\n",
    "- RandomForest: Está muy cerca en performance. Lo usaría como segundo modelo de comparación.\n",
    "- Ridge: Me puede servir para ver cuánto aportan las relaciones no lineales y el feature engineering. Si con nuevas features Ridge mejora mucho, se que estoy agregando información “linealmente útil”.\n",
    "- MLP: Lo descartaría.Con pocos datos y sin tuning claramente está haciendo overfitting o underfitting y con errores gigantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ba90a",
   "metadata": {},
   "source": [
    "Empeizo con el Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e252d",
   "metadata": {},
   "source": [
    "Features a crear:\n",
    "(Armar lista y explicar cada una)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f59bd2",
   "metadata": {},
   "source": [
    "Uso add_basic_features() de Preproccessing.py para agregar las features nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8837cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed/monaco_2025_colapinto_alllaps.csv\")\n",
    "\n",
    "# Aplicar feature engineering v1\n",
    "df_fe = preprocessing.add_basic_features(df)\n",
    "\n",
    "# Target\n",
    "y = df_fe[\"LapTime_s\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08635136",
   "metadata": {},
   "source": [
    "Redefino las columnas a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c98e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LapNumber', 'Stint', 'TyreLife', 'lap_norm_session', 'stint_len',\n",
      "       'stint_lap_index', 'stint_lap_norm', 'tyrelife_norm_stint', 'is_race',\n",
      "       'compound_order', 'Session', 'Compound'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "LEGAL_FEATURES_NUM = [\n",
    "    \"LapNumber\",\n",
    "    \"Stint\",\n",
    "    \"TyreLife\",\n",
    "    \"lap_norm_session\",\n",
    "    \"stint_len\",\n",
    "    \"stint_lap_index\",\n",
    "    \"stint_lap_norm\",\n",
    "    \"tyrelife_norm_stint\",\n",
    "    \"is_race\",\n",
    "    \"compound_order\",\n",
    "]\n",
    "\n",
    "LEGAL_FEATURES_CAT = [\n",
    "    \"Session\",\n",
    "    \"Compound\",\n",
    "]\n",
    "\n",
    "LEGAL_FEATURES_NUM = [c for c in LEGAL_FEATURES_NUM if c in df_fe.columns]\n",
    "LEGAL_FEATURES_CAT = [c for c in LEGAL_FEATURES_CAT if c in df_fe.columns]\n",
    "\n",
    "FEATURES = LEGAL_FEATURES_NUM + LEGAL_FEATURES_CAT\n",
    "X = df_fe[FEATURES].copy()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), LEGAL_FEATURES_NUM),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), LEGAL_FEATURES_CAT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#printeo las columnas de x\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f76d261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoosting_FE_v1] Fold 1: MAE=0.850, RMSE=1.265, R2=0.697\n",
      "[GradientBoosting_FE_v1] Fold 2: MAE=0.321, RMSE=0.397, R2=0.975\n",
      "[GradientBoosting_FE_v1] Fold 3: MAE=0.864, RMSE=1.103, R2=0.789\n",
      "[GradientBoosting_FE_v1] Fold 4: MAE=1.157, RMSE=1.698, R2=0.258\n",
      "[GradientBoosting_FE_v1] Fold 5: MAE=0.532, RMSE=0.692, R2=0.834\n",
      "\n",
      "[GradientBoosting_FE_v1] === Promedio 5 folds ===\n",
      "MAE  medio: 0.745 ± 0.290\n",
      "RMSE medio: 1.031 ± 0.452\n",
      "R2   medio: 0.711 ± 0.243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo base de Gradient Boosting\n",
    "gb_fe = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluamos con KFold usando las nuevas features\n",
    "gb_fe_results = evaluate_model_cv(\n",
    "    name=\"GradientBoosting_FE_v1\",\n",
    "    regressor=gb_fe,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80fb34",
   "metadata": {},
   "source": [
    "Resumen de los Resultados del FE_v1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78b9d1",
   "metadata": {},
   "source": [
    "Todas las metricas empeoraron. Por qué puede haber pasado: \n",
    "- Dataset chico + más features = más riesgo de sobreajuste / ruido\n",
    "- Metimos features muy derivadas de las mismas cosas\n",
    "- Hay features que, desde la lógica del simulador, usan “info del futuro” (Bastante trampa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4abb92",
   "metadata": {},
   "source": [
    "Vamos a hacer una segunda version Feature Engineering v2 bastante mas conservadora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f16f2",
   "metadata": {},
   "source": [
    "Feature Engineering v2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7418190",
   "metadata": {},
   "source": [
    "Elimino estas features “con futuro” del DataFrame y de LEGAL_FEATURES_NUM:\n",
    "- stint_len\n",
    "- stint_lap_index\n",
    "- stint_lap_norm\n",
    "- tyrelife_norm_stint\n",
    "\n",
    "Me quedo con las que conceptualmente sí tienen sentido para el simulador y no duplican demasiado:\n",
    "\n",
    "- lap_norm_session → fase de la sesión (principio/medio/fin).\n",
    "- is_race → modo práctica vs carrera (puede ser útil).\n",
    "- compound_order → codifica “blando vs duro” de forma ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6014c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_FEATURES_NUM = [\n",
    "    \"LapNumber\",\n",
    "    \"Stint\",\n",
    "    \"TyreLife\",\n",
    "    \"lap_norm_session\",\n",
    "    \"is_race\",\n",
    "    \"compound_order\",\n",
    "]\n",
    "\n",
    "LEGAL_FEATURES_CAT = [\n",
    "    \"Session\",\n",
    "    \"Compound\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5db08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LapNumber', 'Stint', 'TyreLife', 'lap_norm_session', 'is_race',\n",
      "       'compound_order', 'Session', 'Compound'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "LEGAL_FEATURES_NUM = [c for c in LEGAL_FEATURES_NUM if c in df_fe.columns]\n",
    "LEGAL_FEATURES_CAT = [c for c in LEGAL_FEATURES_CAT if c in df_fe.columns]\n",
    "\n",
    "FEATURES = LEGAL_FEATURES_NUM + LEGAL_FEATURES_CAT\n",
    "X = df_fe[FEATURES].copy()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), LEGAL_FEATURES_NUM),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), LEGAL_FEATURES_CAT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#printeo las columnas de x\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ec1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoosting_FE_v2] Fold 1: MAE=0.803, RMSE=1.113, R2=0.766\n",
      "[GradientBoosting_FE_v2] Fold 2: MAE=0.408, RMSE=0.522, R2=0.957\n",
      "[GradientBoosting_FE_v2] Fold 3: MAE=0.642, RMSE=0.908, R2=0.856\n",
      "[GradientBoosting_FE_v2] Fold 4: MAE=1.026, RMSE=1.527, R2=0.400\n",
      "[GradientBoosting_FE_v2] Fold 5: MAE=0.548, RMSE=0.641, R2=0.857\n",
      "\n",
      "[GradientBoosting_FE_v2] === Promedio 5 folds ===\n",
      "MAE  medio: 0.685 ± 0.213\n",
      "RMSE medio: 0.942 ± 0.358\n",
      "R2   medio: 0.767 ± 0.193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos con KFold usando las nuevas features\n",
    "gb_fe_results = evaluate_model_cv(\n",
    "    name=\"GradientBoosting_FE_v2\",\n",
    "    regressor=gb_fe,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb64fd7",
   "metadata": {},
   "source": [
    "Resumen de los Resultados para FE V2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c8f5d",
   "metadata": {},
   "source": [
    "- En comparacion con el modelo de GB sin FE se mejora un poco el MAE y el RMSE, manteniendo valor practicamente igual de R2\n",
    "- FE_v2 no rompe nada y ayuda un poco\n",
    "- Las nuevas features son conceptualmente correctas para el simulador\n",
    "- El modelo sigue explicando ~75% de la varianza de LapTime (baseline sólido para comparar estrategias “gruesas” en el simulador y, más adelante, ver si el aprendizaje de (PCA/AE) aporta algo extra.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e93713",
   "metadata": {},
   "source": [
    "Pasos a seguir:\n",
    "- Congelar este setup como baseline oficial: Features: LapNumber, Stint, TyreLife, lap_norm_session, is_race, compound_order, Session, Compound. Modelo: GradientBoosting con los hiperparámetros actuales. Guardar estos resultados (tabla y configuración) para la parte del informe/paper (“Baseline clásico”).\n",
    "-  Hacer tuning ligero de hiperparámetros del GB (n_estimators, max_depth, learning_rate) usando el mismo K-fold.\n",
    "\n",
    "Mas adelante:\n",
    "- armar el pipeline de PCA + clustering sobre las vueltas\n",
    "- diseñar el autoencoder para aprender el espacio latente y ver si aparecen clusters por piloto/compuesto/estado de pista."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (CPU)",
   "language": "python",
   "name": "pytorch_cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
